# Big Data Fraud Detection Pipeline

Há»‡ thá»‘ng pipeline Big Data hoÃ n chá»‰nh sá»­ dá»¥ng Apache Airflow Ä‘á»ƒ Ä‘iá»u phá»‘i, Apache Spark ML Ä‘á»ƒ huáº¥n luyá»‡n vÃ  dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh phÃ¡t hiá»‡n gian láº­n, Apache Kafka Ä‘á»ƒ streaming dá»¯ liá»‡u, vÃ  Apache Hadoop HDFS Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u phÃ¢n tÃ¡n.

## ğŸ“‹ Má»¥c lá»¥c

- [Tá»•ng quan](#tá»•ng-quan)
- [Kiáº¿n trÃºc há»‡ thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [YÃªu cáº§u há»‡ thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [Cáº¥u hÃ¬nh](#cáº¥u-hÃ¬nh)
- [Sá»­ dá»¥ng](#sá»­-dá»¥ng)
- [DAGs](#dags)
- [LÆ°u Ã½](#lÆ°u-Ã½)
- [Troubleshooting](#troubleshooting)
- [TÃ¡c giáº£](#tÃ¡c-giáº£)

---

## ğŸ¯ Tá»•ng quan

Há»‡ thá»‘ng pipeline Big Data Ä‘á»ƒ phÃ¡t hiá»‡n gian láº­n giao dá»‹ch sá»­ dá»¥ng:

- **Apache Airflow**: Äiá»u phá»‘i workflow
- **Apache Spark ML**: Huáº¥n luyá»‡n vÃ  dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh Random Forest
- **Apache Kafka**: Streaming dá»¯ liá»‡u real-time
- **Apache Hadoop HDFS**: LÆ°u trá»¯ dá»¯ liá»‡u phÃ¢n tÃ¡n
- **Celery**: PhÃ¢n phá»‘i tasks trÃªn nhiá»u mÃ¡y

### Chá»©c nÄƒng chÃ­nh

1. Huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest tá»« dá»¯ liá»‡u trÃªn HDFS
2. Dá»± Ä‘oÃ¡n real-time cÃ¡c giao dá»‹ch tá»« Kafka stream
3. Streaming dá»¯ liá»‡u tá»« HDFS vÃ o Kafka
4. Äiá»u phá»‘i toÃ n bá»™ pipeline báº±ng Airflow

### CÃ´ng nghá»‡ sá»­ dá»¥ng

| Component | Version | Má»¥c Ä‘Ã­ch |
|-----------|---------|----------|
| Apache Airflow | 3.1.1 | Workflow orchestration vÃ  scheduling |
| Apache Spark | 4.0.1 | Distributed data processing vÃ  ML |
| Apache Kafka | Latest | Real-time data streaming |
| Apache Hadoop | Latest | Distributed storage (HDFS) |
| Celery | Latest | Distributed task execution |
| Redis | 7.2 | Message broker cho Celery |
| PostgreSQL | 16 | Metadata database cho Airflow |
| Python | 3.10+ | Programming language |

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### SÆ¡ Ä‘á»“ tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Airflow Orchestrator                      â”‚
â”‚                  (192.168.80.98:9090)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Scheduler   â”‚  â”‚ DAG Processorâ”‚  â”‚  Triggerer   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                          â”‚                                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚         â”‚      Celery Executor            â”‚                 â”‚
â”‚         â”‚   (Redis Broker + Workers)      â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hadoop & Sparkâ”‚  â”‚     Kafka      â”‚  â”‚ Spark Worker  â”‚
â”‚    Master     â”‚  â”‚                â”‚  â”‚                â”‚
â”‚ 192.168.80.52 â”‚  â”‚ 192.168.80.122 â”‚  â”‚ 192.168.80.130 â”‚
â”‚               â”‚  â”‚                â”‚  â”‚                â”‚
â”‚ â€¢ HDFS        â”‚  â”‚ â€¢ Input Topic  â”‚  â”‚ â€¢ Prediction   â”‚
â”‚ â€¢ Spark       â”‚  â”‚ â€¢ Output Topic â”‚  â”‚   Processing   â”‚
â”‚   Master      â”‚  â”‚                â”‚  â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### PhÃ¢n bá»‘ mÃ¡y vÃ  vai trÃ²

| IP Address | Vai trÃ² | Services | Celery Queue |
|------------|---------|----------|--------------|
| 192.168.80.98 | Airflow Server | Airflow, PostgreSQL, Redis | default |
| 192.168.80.52 | Hadoop & Spark Master | Hadoop HDFS, Spark Master | node_52 |
| 192.168.80.122 | Kafka & Spark Worker | Kafka Broker, Spark Worker | node_122 |
| 192.168.80.130 | Spark Worker | Spark Worker | node_130 |

### Luá»“ng dá»¯ liá»‡u

```
1. Training Phase:
   HDFS (train.csv) â†’ Spark ML â†’ Model â†’ HDFS (model/)

2. Prediction Phase:
   HDFS (stream.csv) â†’ Kafka (input) â†’ Spark Prediction â†’ Kafka (output)

3. Real-time Processing:
   Kafka (input) â†’ Spark Streaming â†’ Load Model â†’ Predict â†’ Kafka (output)
```

### Dataset vÃ  Model

- **Dataset**: Credit Card Fraud Detection (30 features: Time, V1-V28, Amount)
- **Model**: Random Forest (300 trees, maxDepth=15)
- **Data Location**: `hdfs://192.168.80.52:9000/data/train.csv`
- **Model Location**: `hdfs://192.168.80.52:9000/model`

---

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### YÃªu cáº§u pháº§n cá»©ng

- **Tá»‘i thiá»ƒu**: 4GB RAM, 2 CPU cores, 20GB disk space
- **Khuyáº¿n nghá»‹**: 8GB+ RAM, 4+ CPU cores, 50GB+ disk space

### YÃªu cáº§u pháº§n má»m

- **Docker & Docker Compose** (trÃªn mÃ¡y Airflow)
- **Python 3.10+** vá»›i `uv` hoáº·c `pip` (trÃªn táº¥t cáº£ cÃ¡c mÃ¡y)
- **SSH access** giá»¯a cÃ¡c mÃ¡y (passwordless SSH khuyáº¿n nghá»‹)
- **Java 17+** (cho Spark vÃ  Hadoop)
- **Network connectivity** giá»¯a táº¥t cáº£ cÃ¡c mÃ¡y

### Dependencies

```bash
# Python packages cáº§n thiáº¿t
celery>=5.3.0
redis>=5.0.0
psycopg2-binary>=2.9.0
pyspark>=3.5.0
```

---

## ğŸš€ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone repository

```bash
# Thay tháº¿ <repository-url> báº±ng URL thá»±c táº¿ cá»§a repository
git clone <repository-url>
cd tai_thuy
```

### BÆ°á»›c 2: CÃ i Ä‘áº·t trÃªn mÃ¡y Airflow (192.168.80.98)

```bash
cd airflow-docker

# Táº¡o file .env
echo "AIRFLOW_UID=$(id -u)" > .env

# Khá»Ÿi táº¡o Airflow database
docker compose up airflow-init

# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker compose ps
```

### BÆ°á»›c 3: CÃ i Ä‘áº·t Celery Workers trÃªn cÃ¡c mÃ¡y khÃ¡c

#### TrÃªn mÃ¡y Hadoop & Spark Master (192.168.80.52):

```bash
cd ~/tai_thuy/airflow-docker

# CÃ i Ä‘áº·t dependencies
uv pip install celery redis psycopg2-binary

# Khá»Ÿi Ä‘á»™ng Celery worker
nohup uv run celery -A mycelery.system_worker.app worker \
    --loglevel=INFO -E -Q node_52 \
    > celery_node_52.log 2>&1 &

# Kiá»ƒm tra
pgrep -fl "celery.*worker.*node_52"
```

#### TrÃªn mÃ¡y Kafka (192.168.80.122):

```bash
cd ~/tai_thuy/airflow-docker

# CÃ i Ä‘áº·t dependencies
uv pip install celery redis psycopg2-binary

# Khá»Ÿi Ä‘á»™ng Celery worker
nohup uv run celery -A mycelery.system_worker.app worker \
    --loglevel=INFO -E -Q node_122 \
    > celery_node_122.log 2>&1 &

# Kiá»ƒm tra
pgrep -fl "celery.*worker.*node_122"
```

#### TrÃªn mÃ¡y Spark Worker (192.168.80.130):

```bash
cd ~/tai_thuy/airflow-docker

# CÃ i Ä‘áº·t dependencies
uv pip install celery redis psycopg2-binary

# Khá»Ÿi Ä‘á»™ng Celery worker
nohup uv run celery -A mycelery.system_worker.app worker \
    --loglevel=INFO -E -Q node_130 \
    > celery_node_130.log 2>&1 &

# Kiá»ƒm tra
pgrep -fl "celery.*worker.*node_130"
```

### BÆ°á»›c 4: Chuáº©n bá»‹ dá»¯ liá»‡u trÃªn HDFS

```bash
# SSH vÃ o mÃ¡y Hadoop (192.168.80.52)
ssh labsit@192.168.80.52

# Táº¡o thÆ° má»¥c trÃªn HDFS
hdfs dfs -mkdir -p /data
hdfs dfs -mkdir -p /model
hdfs dfs -mkdir -p /checkpoints

# Upload dá»¯ liá»‡u training vÃ  streaming
hdfs dfs -put ~/tai_thuy/train_model/train.csv /data/train.csv
hdfs dfs -put ~/tai_thuy/streaming/stream.csv /data/stream.csv

# Kiá»ƒm tra
hdfs dfs -ls /data
```

### BÆ°á»›c 5: Kiá»ƒm tra káº¿t ná»‘i

```bash
# Tá»« mÃ¡y Airflow, kiá»ƒm tra Celery workers
cd ~/tai_thuy/airflow-docker
docker compose exec airflow-scheduler airflow celery list-workers

# Káº¿t quáº£ mong Ä‘á»£i:
# worker_name          | queues  
# =====================+=========
# celery@<hostname>    | default 
# celery@<hostname>    | node_52 
# celery@<hostname>    | node_122
# celery@<hostname>    | node_130
```

---

## âš™ï¸ Cáº¥u hÃ¬nh

### Cáº¥u hÃ¬nh Airflow

File `airflow-docker/docker-compose.yaml` Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i:

- **Executor**: CeleryExecutor
- **Broker**: Redis táº¡i `192.168.80.98:6379`
- **Database**: PostgreSQL táº¡i `192.168.80.98:5432`
- **Web UI**: Port `9090`

### Cáº¥u hÃ¬nh Celery Workers

File `airflow-docker/mycelery/system_worker.py` chá»©a:

- **Broker URL**: `redis://192.168.80.98:6379/0`
- **Backend**: `db+postgresql://airflow:airflow@192.168.80.98/airflow`
- **Queues**: `node_52`, `node_122`, `node_130`

### Cáº¥u hÃ¬nh Pipeline

File `airflow-docker/dags/bigdata_full_pipeline_dag.py` chá»©a cáº¥u hÃ¬nh:

```python
FULL_PIPELINE_CONFIG = {
    'hadoop_host': '192.168.80.52',
    'spark_master_url': 'spark://192.168.80.52:7077',
    'kafka_bootstrap': '192.168.80.122:9092',
    'train_input': 'hdfs://192.168.80.52:9000/data/train.csv',
    'model_path': 'hdfs://192.168.80.52:9000/model',
    # ... cÃ¡c cáº¥u hÃ¬nh khÃ¡c
}
```

---

## ğŸ“– Sá»­ dá»¥ng

### Truy cáº­p Airflow Web UI

1. Má»Ÿ trÃ¬nh duyá»‡t vÃ  truy cáº­p: `http://192.168.80.98:9090`
2. ÄÄƒng nháº­p vá»›i:
   - Username: `airflow`
   - Password: `airflow`

### Cháº¡y Full Pipeline

1. Trong Airflow UI, tÃ¬m DAG `bigdata_full_pipeline`
2. Click vÃ o DAG vÃ  chá»n **"Trigger DAG"**
3. Pipeline sáº½ tá»± Ä‘á»™ng:
   - Khá»Ÿi Ä‘á»™ng infrastructure (Hadoop, Spark, Kafka)
   - Kiá»ƒm tra services sáºµn sÃ ng
   - Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»« HDFS
   - Táº¡o Kafka topics (input, output)
   - Khá»Ÿi Ä‘á»™ng Spark prediction job
   - Chá» 60 giÃ¢y
   - Khá»Ÿi Ä‘á»™ng streaming job tá»« HDFS vÃ o Kafka

### Parameters

DAG há»— trá»£ cÃ¡c parameters:

- `start_infrastructure` (boolean): Khá»Ÿi Ä‘á»™ng infrastructure hay khÃ´ng
- `train_model` (boolean): Huáº¥n luyá»‡n mÃ´ hÃ¬nh hay khÃ´ng
- `start_predict` (boolean): Khá»Ÿi Ä‘á»™ng prediction job hay khÃ´ng
- `start_streaming` (boolean): Khá»Ÿi Ä‘á»™ng streaming job hay khÃ´ng
- `delay_before_streaming` (integer): Delay trÆ°á»›c khi streaming (máº·c Ä‘á»‹nh: 60 giÃ¢y)

### Xem logs

```bash
# Logs Airflow Scheduler
cd ~/tai_thuy/airflow-docker
docker compose logs -f airflow-scheduler

# Logs Celery Workers trÃªn cÃ¡c mÃ¡y khÃ¡c
tail -f ~/tai_thuy/airflow-docker/celery_node_52.log
tail -f ~/tai_thuy/airflow-docker/celery_node_122.log
tail -f ~/tai_thuy/airflow-docker/celery_node_130.log

# Logs Spark jobs
tail -f /tmp/spark_predict.log
tail -f /tmp/spark_kafka_streaming.log
```

---

## ğŸ›ï¸ Kiáº¿n trÃºc

### Celery Queues

Dá»± Ã¡n sá»­ dá»¥ng **IP-based Celery queues** Ä‘á»ƒ phÃ¢n phá»‘i tasks Ä‘áº¿n Ä‘Ãºng mÃ¡y:

- `node_52`: Hadoop & Spark Master (192.168.80.52)
- `node_122`: Kafka & Spark Worker (192.168.80.122)
- `node_130`: Spark Worker (192.168.80.130)

Má»—i mÃ¡y cáº§n cháº¡y Celery worker vá»›i queue tÆ°Æ¡ng á»©ng Ä‘á»ƒ nháº­n tasks tá»« Airflow.

---

## ğŸ“ DAGs

### DAG chÃ­nh

#### `bigdata_full_pipeline`

DAG chÃ­nh thá»±c hiá»‡n toÃ n bá»™ pipeline tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i:

**Phases:**
1. **Infrastructure Setup**: Khá»Ÿi Ä‘á»™ng Hadoop, Spark Master, Spark Workers, Kafka
2. **Service Verification**: Kiá»ƒm tra táº¥t cáº£ services Ä‘Ã£ sáºµn sÃ ng
3. **Model Training**: Huáº¥n luyá»‡n Random Forest model tá»« HDFS
4. **Model Verification**: XÃ¡c minh model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u
5. **Kafka Topics Setup**: Táº¡o topics input vÃ  output náº¿u chÆ°a cÃ³
6. **Prediction Job**: Khá»Ÿi Ä‘á»™ng Spark streaming prediction job
7. **Wait Period**: Chá» 60 giÃ¢y Ä‘á»ƒ prediction job sáºµn sÃ ng
8. **Streaming Job**: Khá»Ÿi Ä‘á»™ng streaming tá»« HDFS vÃ o Kafka

**Dependencies:**
```
start_hadoop
  â†“
start_spark_master â†’ [start_spark_worker_1, start_spark_worker_2]
  â†“
start_kafka
  â†“
[check_hadoop_ready, check_spark_ready, check_kafka_ready]
  â†“
train_model â†’ verify_model_saved
  â†“
check_kafka_topics
  â†“
start_predict
  â†“
wait_before_streaming
  â†“
start_streaming
```

### DAGs test

CÃ¡c DAG test Ä‘á»ƒ kiá»ƒm tra tá»«ng component riÃªng láº»:

- `test_hadoop_dag`: Test Hadoop start/stop
- `test_spark_dag`: Test Spark cluster
- `test_kafka_dag`: Test Kafka start/stop
- `test_train_model_dag`: Test model training
- `test_spark_predict_dag`: Test prediction job
- `test_kafka_streaming_dag`: Test streaming tá»« HDFS vÃ o Kafka
- `test_create_kafka_topics_dag`: Test táº¡o Kafka topics

---

## âš ï¸ LÆ°u Ã½

- Cáº§n khá»Ÿi Ä‘á»™ng Celery workers trÃªn táº¥t cáº£ cÃ¡c mÃ¡y trÆ°á»›c khi cháº¡y DAG
- Äáº£m báº£o network connectivity giá»¯a cÃ¡c mÃ¡y
- Dá»¯ liá»‡u pháº£i Ä‘Æ°á»£c upload lÃªn HDFS trÆ°á»›c khi training
- Kafka topics sáº½ Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng khi start Kafka

---

## ğŸ”§ Troubleshooting

### Váº¥n Ä‘á»: Airflow khÃ´ng nháº­n Ä‘Æ°á»£c tasks tá»« Celery workers

**NguyÃªn nhÃ¢n**: Celery workers trÃªn cÃ¡c mÃ¡y khÃ¡c chÆ°a Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng hoáº·c khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c Redis.

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra Celery workers
docker compose exec airflow-scheduler airflow celery list-workers

# Náº¿u thiáº¿u worker, khá»Ÿi Ä‘á»™ng láº¡i trÃªn mÃ¡y tÆ°Æ¡ng á»©ng
# VÃ­ dá»¥ trÃªn mÃ¡y 192.168.80.122:
ssh labsit@192.168.80.122
cd ~/tai_thuy/airflow-docker
nohup uv run celery -A mycelery.system_worker.app worker \
    --loglevel=INFO -E -Q node_122 \
    > celery_node_122.log 2>&1 &
```

### Váº¥n Ä‘á»: DAG khÃ´ng xuáº¥t hiá»‡n trong Airflow UI

**NguyÃªn nhÃ¢n**: Lá»—i syntax hoáº·c import trong DAG file.

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra lá»—i import
docker compose exec airflow-scheduler airflow dags list-import-errors

# Kiá»ƒm tra syntax Python
docker compose exec airflow-scheduler python3 -m py_compile dags/bigdata_full_pipeline_dag.py
```

### Váº¥n Ä‘á»: Spark job khÃ´ng cháº¡y

**NguyÃªn nhÃ¢n**: Spark Master khÃ´ng sáºµn sÃ ng hoáº·c khÃ´ng Ä‘á»§ resources.

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra Spark Master UI: http://192.168.80.52:8080
# Kiá»ƒm tra workers Ä‘Ã£ káº¿t ná»‘i chÆ°a
# Kiá»ƒm tra logs Spark job
tail -f /tmp/spark_predict.log
```

### Váº¥n Ä‘á»: Kafka topics khÃ´ng Ä‘Æ°á»£c táº¡o

**NguyÃªn nhÃ¢n**: Kafka chÆ°a sáºµn sÃ ng hoáº·c lá»—i káº¿t ná»‘i.

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra Kafka Ä‘ang cháº¡y
ssh labsit@192.168.80.122
docker ps | grep kafka

# Táº¡o topics thá»§ cÃ´ng náº¿u cáº§n
docker exec -i kafka kafka-topics \
    --create \
    --topic input \
    --bootstrap-server 192.168.80.122:9092 \
    --partitions 1 \
    --replication-factor 1
```

### Váº¥n Ä‘á»: HDFS khÃ´ng accessible

**NguyÃªn nhÃ¢n**: Hadoop chÆ°a khá»Ÿi Ä‘á»™ng hoáº·c lá»—i cáº¥u hÃ¬nh.

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra Hadoop services
ssh labsit@192.168.80.52
jps | grep -E "NameNode|DataNode"

# Kiá»ƒm tra HDFS
hdfs dfsadmin -report
hdfs dfs -ls /
```

### Kiá»ƒm tra káº¿t ná»‘i Redis

```bash
# Tá»« mÃ¡y Airflow
docker compose exec redis redis-cli PING

# Tá»« cÃ¡c mÃ¡y khÃ¡c
redis-cli -h 192.168.80.98 -p 6379 PING
```

### Xem logs chi tiáº¿t

```bash
# Airflow logs
docker compose logs -f airflow-scheduler
docker compose logs -f airflow-worker

# Celery worker logs trÃªn cÃ¡c mÃ¡y
tail -f ~/tai_thuy/airflow-docker/celery_node_*.log

# Spark logs
tail -f /tmp/spark_*.log
```

---

## ğŸ“Š Monitoring

### Airflow Web UI

- **URL**: http://192.168.80.98:9090
- **Features**: Xem DAGs, task status, logs, graphs

### Spark Master UI

- **URL**: http://192.168.80.52:8080
- **Features**: Xem Spark cluster status, applications, workers

### Kafka Topics

```bash
# List topics
docker exec -i kafka kafka-topics \
    --list \
    --bootstrap-server 192.168.80.122:9092

# Xem messages trong topic
docker exec -i kafka kafka-console-consumer \
    --bootstrap-server 192.168.80.122:9092 \
    --topic output \
    --from-beginning
```

### HDFS Status

```bash
# Xem cluster status
hdfs dfsadmin -report

# Xem disk usage
hdfs dfs -du -h /
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
tai_thuy/
â”œâ”€â”€ airflow-docker/              # Airflow vÃ  Celery configuration
â”‚   â”œâ”€â”€ dags/                    # DAG definitions
â”‚   â”‚   â”œâ”€â”€ bigdata_full_pipeline_dag.py
â”‚   â”‚   â”œâ”€â”€ test_*.py
â”‚   â”œâ”€â”€ mycelery/                # Celery tasks
â”‚   â”‚   â””â”€â”€ system_worker.py
â”‚   â”œâ”€â”€ config/                  # Airflow config
â”‚   â”œâ”€â”€ logs/                    # Airflow logs
â”‚   â”œâ”€â”€ docker-compose.yaml      # Docker services
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ train_model/                 # Model training scripts
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ predict/                     # Prediction scripts
â”‚   â””â”€â”€ predict_fraud.py
â”œâ”€â”€ streaming/                   # Streaming scripts
â”‚   â”œâ”€â”€ kafka_streaming.py
â”‚   â””â”€â”€ stream.csv
â””â”€â”€ README.md                    # This file
```

---

## ğŸ› ï¸ Development

### ThÃªm task má»›i vÃ o Celery

1. ThÃªm function vÃ o `airflow-docker/mycelery/system_worker.py`:

```python
@app.task(bind=True)
def my_new_task(self, param1, param2):
    """MÃ´ táº£ task"""
    # Implementation
    return {'status': 'success', 'result': ...}
```

2. Import trong DAG:

```python
from mycelery.system_worker import my_new_task
```

3. Sá»­ dá»¥ng trong DAG:

```python
result = my_new_task.apply_async(
    args=[param1, param2],
    queue='node_52'  # Chá»n queue phÃ¹ há»£p
)
```

### Táº¡o DAG má»›i

1. Táº¡o file má»›i trong `airflow-docker/dags/`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

with DAG(
    dag_id='my_new_dag',
    start_date=datetime(2024, 1, 1),
    schedule=None,
) as dag:
    # Define tasks
    pass
```

2. Airflow sáº½ tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  load DAG má»›i.

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Apache Hadoop Documentation](https://hadoop.apache.org/docs/)
- [Celery Documentation](https://docs.celeryq.dev/)

---

## ğŸ‘¥ TÃ¡c giáº£

**NhÃ³m 2 - Khoa CÃ´ng nghá»‡ ThÃ´ng tin**

- **Phan VÄƒn TÃ i** (2202081)
- **Phan Minh Thuy** (2202079)

**Giáº£ng viÃªn hÆ°á»›ng dáº«n**: Dr. Cao Tien Dung

**TrÆ°á»ng**: Äáº¡i há»c TÃ¢n Táº¡o  
**Khoa**: CÃ´ng nghá»‡ ThÃ´ng tin

---

## ğŸ“„ License

Apache License 2.0

---

## Acknowledgments

- Apache Software Foundation cho cÃ¡c cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ
- Cá»™ng Ä‘á»“ng open source
- Dr. Cao Tien Dung cho sá»± hÆ°á»›ng dáº«n vÃ  há»— trá»£

---

**LÆ°u Ã½**: ÄÃ¢y lÃ  dá»± Ã¡n há»c táº­p. Äá»ƒ sá»­ dá»¥ng trong production, cáº§n thÃªm cÃ¡c biá»‡n phÃ¡p báº£o máº­t, monitoring, vÃ  backup phÃ¹ há»£p.

